# ğŸ“˜ Multilingual RAG System

A high-performance, multilingual Retrieval-Augmented Generation (RAG) system combining FAISS, Groq LLaMA models, Tavily web search, and robust preprocessing + corrective filtering to deliver fast, accurate, and context-grounded answers.

Designed as part of an advanced AI hiring task, this system demonstrates production-level architectural design, retrieval quality control, and multilingual capability.

# ğŸŒŸ Features

Multilingual Query Support
Automatically detects and processes queries in multiple languages.

High-speed Vector Retrieval using FAISS
Stores embeddings locally for fast, offline similarity search.

Groq-accelerated LLaMA Generation
Ultra-fast inference with LLaMA 3.x on Groq API.

Preprocessing Pipeline
Includes text cleaning, chunking, metadata processing, and embeddings generation.

Corrective Filtering Stages

Query preprocessing

Response filtering

Retrieval ranking

Metadata alignment

Context re-weighting

Local Caching System
Uses exact_cache.json for performance optimizations.

Web Search Fallback (Tavily)
When vector retrieval is insufficient or unclear, the system expands context using external web search.

Full FastAPI Backend
Clean REST API with clear request/response schemas.

ElevenLabs TTS Integration for responses

Lightweight HTML Frontend
Inline CSS+JS for simple interaction with the RAG backend.

# ğŸ§  Question Difficulty Classification Model (spaCy) for metadata

Inside the models/ directory, the project includes a custom fine-tuned spaCy text classification model.
This model predicts the difficulty level of a user question across five classes:

Too Easy
Easy
Medium
Hard
Too Hard

ğŸ“š Fine-Tuning Data

The model was fine-tuned using a Kaggle dataset of natural-language questions.
Before training, the dataset was cleaned and standardized inside the data/processed/ folder.

# ğŸ§ª Fine-Tuning Notebook

The full training workflowâ€”including preprocessing, labeling, fine-tuning, and evaluationâ€”is documented in the Jupyter notebook located inside notebooks folder

# ğŸ— Architecture

Overall multi-stage flow inspired by advanced modern RAG systems:

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚       User Query        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  Query Preprocessing   â”‚
                     â”‚  (language, cleaning)  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      FAISS Vector Retrieval            â”‚
              â”‚ (top-k semantically similar chunks)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Retrieval Ranking   â”‚
                    â”‚ + Metadata Filtering â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚   Groq LLaMA Gen    â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ Response Filtering   â”‚
                         â”‚ (grounding + quality)â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â–¼                          â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Final Answer   â”‚       â”‚ Tavily Web Search   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â–¼
                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”‚  Re-run Generation + QC  â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# ğŸ“ Project Structure (Real Directory Tree)

Based on your screenshot:

rag_system/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ elevenlabs_tts.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ service.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ chunks/
â”‚   â”‚   â””â”€â”€ semantic_chunked_dataset.json
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”œâ”€â”€ faiss_index.bin
â”‚   â”‚   â””â”€â”€ metadata_mapping.json
â”‚   â”œâ”€â”€ performance/
â”‚   â”‚   â””â”€â”€ exact_cache.json
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ Natural-Questions-Cleaned.csv
â”‚   â”‚   â””â”€â”€ Natural-Questions-Filtered.csv
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ Natural-Questions-Filtered.csv
â”‚
â”œâ”€â”€ models/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ fine_tune_question_difficulty_spacy.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ embedding.py
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ chunking.py
â”‚   â”‚   â”œâ”€â”€ load_data.py
â”‚   â”‚   â”œâ”€â”€ metadata.py
â”‚   â”‚   â””â”€â”€ preprocess.py
â”‚   â”œâ”€â”€ rag_core/
â”‚   â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lm_integration.py
â”‚   â”‚   â”œâ”€â”€ performance_utils.py
â”‚   â”‚   â”œâ”€â”€ query_preprocessing_v1.0.py
â”‚   â”‚   â”œâ”€â”€ query_preprocessing.py
â”‚   â”‚   â”œâ”€â”€ response_filtering_v1.0.py
â”‚   â”‚   â”œâ”€â”€ response_filtering.py
â”‚   â”‚   â”œâ”€â”€ response_filtering_draft.py
â”‚   â”‚   â””â”€â”€ retrieval_ranking.py
â”‚   â””â”€â”€ retrieval/
â”‚       â”œâ”€â”€ __pycache__/
â”‚       â””â”€â”€ retriever.py
â”‚
â””â”€â”€ static/
    â””â”€â”€ index.html  (inline CSS + JS)

# ğŸ“‹ Prerequisites

Python 3.10+

FastAPI

FAISS

Groq API Key

Tavily API Key

Pydantic

Uvicorn

# ğŸš€ Installation
1. Clone Project
git clone repo.
cd rag_system

2. Create Virtual Environment
python -m venv venv
source venv/bin/activate

3. Install Dependencies
pip install -r requirements.txt

4. Set Environment Variables

Create .env:

GROQ_API_KEY=your_key
TAVILY_API_KEY=your_key
ELEVENLABS_API_KEY=your_key 

MODEL_NAME=llama-3.3-70b-versatile
Copy env_template.txt to .env and fill in your real API keys and paths before running the project.

# â–¶ï¸ Running the Backend

Start FastAPI server:

uvicorn backend.app:app --reload


The API will run at:

http://127.0.0.1:8000

PORT 8000 (optional you can try any available port)
Open the frontend:

rag_system/static/index.html

# ğŸ¯ API Endpoints
POST /ask-question
GET /health
Send a question and receive an answer:

{
  "query": "Who's the Weeknd?!"
}

Response
{
  "answer": "...",
  "context_used": [...],
  "retrieval_time_s": 13
}

ğŸ” How It Works
ğŸ”¹ 1. Query Preprocessing

Cleans text, normalizes multilingual input, and extracts intent.

ğŸ”¹ 2. FAISS Retrieval

Loads faiss_index.bin + metadata_mapping.json for fast vector search.

ğŸ”¹ 3. Document Ranking

Scores retrieved chunks using semantic + metadata rules.

ğŸ”¹ 4. LLaMA Generation

Uses Groq LLaMA for extremely fast inference.

ğŸ”¹ 5. Response Filtering

Multiple safety and grounding checks before final answer.

ğŸ”¹ 6. Tavily Fallback

Searches the web if retrieved knowledge is insufficient.

# ğŸ§ª Example Usage (Python)
from backend.service import RAGService

rag = RAGService()

result = rag.run_query("Explain photosynthesis in simple terms")

print(result.answer)

# ğŸ›  Troubleshooting
âŒ FAISS index not found

Rebuild embeddings:

python src/embeddings/embedding.py

âŒ Slow responses

Delete cache to refresh:

data/performance/exact_cache.json

âŒ Tavily key missing

Double-check .env

# ğŸ¨ Frontend

Your frontend uses:

Pure HTML

Inline CSS

Inline JS (fetch â†’ POST â†’ render response)

Works directly with FastAPI.

# ğŸ“ˆ Performance Notes

Local caching drastically improves repeated queries

FAISS index keeps retrieval <10ms

Groq inference delivers ~10â€“20 tokens/ms

Chunk size & overlap tuned for multilingual content

Preprocessing pipeline reduces noise and improves grounding

# ğŸ§© Future Expansions

Add streaming responses: Enable partial responses for faster user feedback on long outputs.

Add JWT-authenticated endpoints: Secure API access with token-based authentication.

Optimize semantic retrieval latency: Explore approximate nearest neighbor (ANN) indexing, vector pruning, or lightweight embedding models to reduce response time without sacrificing accuracy.

Adaptive semantic caching: Store embeddings or intermediate results for frequently asked queries to reduce repeated computation.

Batch processing for embedding queries: Process multiple queries together to leverage vectorization and speed up retrieval.

Parallelize expensive operations: Use async or multithreading where safe for model inference or external API calls.

Hybrid retrieval: Combine dense vector search with selective metadata filtering to limit candidate results early and speed up overall latency.